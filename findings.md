# Findings

## Research
- Goal: Create a local LLM Testcase generator with Ollama.

## Discoveries
- **Integration**: Ollama API.
- **Model**: `gemma3:1b`.
- **Stack**: Vanilla HTML/JS/CSS (Frontend) + Java Spring Boot (Backend).
- **Data**: Strict JSON output.

## Constraints
- Must run locally.
- **Frontend**: No frameworks (React/Vite forbidden).
- **Backend**: Java (Spring Boot).
- **Storage**: Stateless (No DB).
- **Source of Truth**: User Input.
